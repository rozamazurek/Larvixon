import pandas as pd
import re
import os
from collections import defaultdict

def analyze_metadata_and_annotations(file_path, collect_summary=None):
    """
    Analizuje plik CSV zawierajƒÖcy nazwy plik√≥w wideo/segment√≥w
    i ich adnotacje pod kƒÖtem jako≈õci i sp√≥jno≈õci.
    Mo≈ºe r√≥wnie≈º zbieraƒá dane do podsumowania globalnego.

    Args:
        file_path (str): ≈öcie≈ºka do pliku CSV z metadanami i adnotacjami.
        collect_summary (dict, optional): S≈Çownik do zbierania danych sumarycznych.
                                         Je≈õli None, analiza jest wykonywana bez zbierania podsumowania.
    Returns:
        bool: True, je≈õli analiza pliku przebieg≈Ça pomy≈õlnie, False w przeciwnym razie.
    """
    file_name = os.path.basename(file_path)
    print(f"\n--- ROZPOCZƒòCIE ANALIZY PLIKU: {file_name} ---")
    
    analysis_successful = False

    if not os.path.exists(file_path):
        print(f"B≈ÅƒÑD: Plik nie zosta≈Ç znaleziony pod ≈õcie≈ºkƒÖ: {file_path}")
        print("Upewnij siƒô, ≈ºe ≈õcie≈ºka jest poprawna.")
        print("--- ANALIZA ZAKO≈ÉCZONA Z B≈ÅƒòDEM ---")
        if collect_summary is not None:
            collect_summary['files_processed_with_errors'].append(file_name)
        return analysis_successful

    try:
        df = pd.read_csv(file_path)
        print(f"Plik wczytany. Kszta≈Çt danych: {df.shape}")

        if collect_summary is not None:
            collect_summary['total_files_found'] += 1
            collect_summary['total_rows'] += df.shape[0]
            collect_summary['total_columns'] += df.shape[1]

        expected_columns = ['names', 'annotation']
        if not all(col in df.columns for col in expected_columns):
            print(f"OSTRZE≈ªENIE: Oczekiwane kolumny '{expected_columns}' nie zosta≈Çy znalezione.")
            print(f"Dostƒôpne kolumny: {df.columns.tolist()}")
            if collect_summary is not None:
                collect_summary['files_with_column_warnings'].append(file_name)
        
        # --- 1. Sprawdzenie brakujƒÖcych warto≈õci ---
        print("\n--- 1. Analiza brakujƒÖcych warto≈õci ---")
        missing_values = df.isnull().sum()
        if missing_values.sum() > 0:
            print("Znaleziono brakujƒÖce warto≈õci:")
            print(missing_values[missing_values > 0])
            print("Warto≈õci NaN w kolumnie 'annotation' mogƒÖ wskazywaƒá na brak adnotacji dla danego segmentu.")
            if collect_summary is not None:
                collect_summary['files_with_missing_values'].append(file_name)
                for col, count in missing_values[missing_values > 0].items():
                    collect_summary['missing_values_by_column'][col] += count
        else:
            print("Brak brakujƒÖcych warto≈õci w pliku. ‚úÖ")

        # --- 2. Analiza adnotacji behawioralnych ---
        if 'annotation' in df.columns:
            print("\n--- 2. Analiza adnotacji behawioralnych ---")
            
            df['annotation'] = df['annotation'].astype(str).str.strip() # Upewnij siƒô, ≈ºe to stringi

            unique_annotations = df['annotation'].dropna().unique()
            print(f"Unikalne klasy zachowa≈Ñ: {unique_annotations.tolist()}")
            print("\nRozk≈Çad klas zachowa≈Ñ:")
            annotation_counts = df['annotation'].value_counts(dropna=False)
            print(annotation_counts)

            if collect_summary is not None:
                for ann, count in annotation_counts.items():
                    collect_summary['total_annotations_by_class'][ann] += count

            if len(unique_annotations) > 1:
                min_count = annotation_counts.min()
                max_count = annotation_counts.max()
                ratio = max_count / min_count if min_count > 0 else float('inf')
                
                print(f"\nNajmniej liczna klasa ({annotation_counts.idxmin()}): {min_count} wystƒÖpie≈Ñ")
                print(f"Najliczniejsza klasa ({annotation_counts.idxmax()}): {max_count} wystƒÖpie≈Ñ")
                if ratio > 5:
                    print(f"OSTRZE≈ªENIE: Klasy sƒÖ **mocno niezbalansowane** (stosunek {ratio:.1f}:1). Mo≈ºe to wp≈ÇynƒÖƒá na trening modeli ML. ‚ö†Ô∏è")
                    if collect_summary is not None:
                        collect_summary['files_with_highly_imbalanced_classes'].append(file_name)
                elif ratio > 2:
                    print(f"OSTRZE≈ªENIE: Klasy sƒÖ **niezbalansowane** (stosunek {ratio:.1f}:1). Warto rozwa≈ºyƒá techniki balansowania danych. ‚ö†Ô∏è")
                    if collect_summary is not None:
                        collect_summary['files_with_imbalanced_classes'].append(file_name)
                else:
                    print("Klasy sƒÖ w miarƒô zbalansowane. ‚úÖ")
            else:
                print("Tylko jedna unikalna klasa adnotacji lub brak adnotacji. Brak zr√≥≈ºnicowania.")
            
            problematic_annotations = df[df['annotation'].astype(str).str.contains(',')]
            if not problematic_annotations.empty:
                print("\nOSTRZE≈ªENIE: Znaleziono niesp√≥jne formatowanie adnotacji (np. wiele adnotacji w jednej kom√≥rce): ‚ö†Ô∏è")
                print(problematic_annotations)
                print("Mo≈ºe to wymagaƒá rƒôcznej weryfikacji lub poprawy pliku ≈∫r√≥d≈Çowego.")
                if collect_summary is not None:
                    collect_summary['files_with_problematic_annotations'].append(file_name)
        else:
            print("Kolumna 'annotation' nie zosta≈Ça znaleziona. Pomijam analizƒô adnotacji behawioralnych.")
            if collect_summary is not None:
                collect_summary['files_missing_annotation_column'].append(file_name)


        # --- 3. Analiza metadanych z nazw plik√≥w (`names` column) ---
        if 'names' in df.columns:
            print("\n--- 3. Analiza metadanych z nazw plik√≥w ---")

            df['names'] = df['names'].astype(str) # Upewnij siƒô, ≈ºe to stringi

            df['well_id'] = df['names'].str.extract(r'_well_([A-Z]\d+)')
            if not df['well_id'].isnull().all():
                unique_well_ids = df['well_id'].dropna().unique().tolist()
                print(f"Unikalne ID studzienek: {unique_well_ids}")
                if collect_summary is not None:
                    collect_summary['unique_well_ids'].update(unique_well_ids)
                if df['well_id'].nunique() > 1:
                    print("Liczba segment√≥w na studzienkƒô:")
                    well_counts = df['well_id'].value_counts()
                    print(well_counts)
                    if collect_summary is not None:
                        for well, count in well_counts.items():
                            collect_summary['total_segments_by_well'][well] += count
            else:
                print("Nie uda≈Ço siƒô wyodrƒôbniƒá ID studzienek. Sprawd≈∫ format nazewnictwa.")
                if collect_summary is not None:
                    collect_summary['files_failed_well_id_extraction'].append(file_name)

            def extract_frame_range(filename):
                match = re.search(r'frames_(\d+)_to_(\d+)', str(filename))
                if match:
                    return int(match.group(1)), int(match.group(2))
                return None, None

            df[['start_frame', 'end_frame']] = df['names'].apply(
                lambda x: pd.Series(extract_frame_range(x))
            )

            if not df['start_frame'].isnull().all() and not df['end_frame'].isnull().all():
                print("\nAnaliza ciƒÖg≈Ço≈õci segment√≥w wideo:")
                for well_id, group in df.groupby('well_id'):
                    sorted_group = group.sort_values('start_frame').reset_index(drop=True)
                    
                    if sorted_group.duplicated(['names']).any():
                        print(f"OSTRZE≈ªENIE dla {well_id}: Znaleziono zduplikowane wpisy segment√≥w! ‚ö†Ô∏è")
                        print(sorted_group[sorted_group.duplicated(['names'], keep=False)])
                        if collect_summary is not None:
                            collect_summary['files_with_duplicated_segments'].append(file_name)

                    gaps_found = False
                    for i in range(1, len(sorted_group)):
                        prev_end = sorted_group.loc[i-1, 'end_frame']
                        current_start = sorted_group.loc[i, 'start_frame']
                        
                        if current_start > prev_end + 60:
                            print(f"OSTRZE≈ªENIE dla {well_id}: Luka lub nak≈Çadanie siƒô klatek miƒôdzy segmentami!")
                            print(f"Poprzedni koniec: {prev_end}, Aktualny poczƒÖtek: {current_start}")
                            print(f"Segmenty: {sorted_group.loc[i-1, 'names']} oraz {sorted_group.loc[i, 'names']} ‚ö†Ô∏è")
                            gaps_found = True
                    if gaps_found and collect_summary is not None:
                        collect_summary['files_with_frame_gaps'].append(file_name)
                    if not gaps_found:
                        print(f"Dla {well_id}: Segmenty klatek sƒÖ ciƒÖg≈Çe i bez luk. ‚úÖ")
            else:
                print("Nie uda≈Ço siƒô wyodrƒôbniƒá zakres√≥w klatek z nazw plik√≥w. Pomijam analizƒô ciƒÖg≈Ço≈õci.")
                if collect_summary is not None:
                    collect_summary['files_failed_frame_extraction'].append(file_name)

            df['date'] = df['names'].str.extract(r'^(\d{8})')
            df['age_dpf'] = df['names'].str.extract(r'_(\d+)DPF_')
            df['experiment_type'] = df['names'].str.extract(r'_(AB_PTZ|Control)_')

            print("\nPrzyk≈Çadowe wyodrƒôbnione metadane:")
            print(df[['well_id', 'date', 'age_dpf', 'experiment_type']].head())

            print("\nBrakujƒÖce warto≈õci w wyodrƒôbnionych metadanych:")
            extracted_metadata_missing = df[['well_id', 'date', 'age_dpf', 'experiment_type']].isnull().sum()
            print(extracted_metadata_missing)
            if collect_summary is not None:
                for col, count in extracted_metadata_missing[extracted_metadata_missing > 0].items():
                    collect_summary['missing_extracted_metadata'][col] += count
        else:
            print("Kolumna 'names' nie zosta≈Ça znaleziona. Pomijam analizƒô metadanych z nazw plik√≥w.")
            if collect_summary is not None:
                collect_summary['files_missing_names_column'].append(file_name)
        
        analysis_successful = True

    except pd.errors.EmptyDataError:
        print(f"B≈ÅƒÑD: Plik '{file_path}' jest pusty.")
        if collect_summary is not None:
            collect_summary['files_empty'].append(file_name)
    except Exception as e:
        print(f"WystƒÖpi≈Ç nieoczekiwany b≈ÇƒÖd podczas analizy: {e}")
        if collect_summary is not None:
            collect_summary['files_processed_with_errors'].append(file_name)

    print(f"--- ANALIZA PLIKU: {file_name} ZAKO≈ÉCZONA ---")
    return analysis_successful

def summarize_project_data_quality(base_dir):
    """
    Przeszukuje podfoldery w podanej ≈õcie≈ºce, znajduje pliki CSV
    pasujƒÖce do wzorca 'dataset*.csv' i wykonuje na nich analizƒô jako≈õci danych.
    Zbiera sumaryczne informacje o jako≈õci danych w ca≈Çym projekcie.

    Args:
        base_dir (str): ≈öcie≈ºka do g≈Ç√≥wnego katalogu projektu.
    """
    print(f"*** ROZPOCZƒòCIE ANALIZY JAKO≈öCI DANYCH DLA PROJEKTU W: {base_dir} ***")

    if not os.path.isdir(base_dir):
        print(f"B≈ÅƒÑD: Katalog '{base_dir}' nie istnieje lub nie jest katalogiem.")
        print("--- ANALIZA PROJEKTU ZAKO≈ÉCZONA Z B≈ÅƒòDEM ---")
        return

    # S≈Çownik do zbierania danych sumarycznych
    project_summary = {
        'total_files_found': 0,
        'total_files_processed': 0,
        'total_rows': 0,
        'total_columns': 0,
        'files_processed_with_errors': [],
        'files_empty': [],
        'files_with_column_warnings': [],
        'files_with_missing_values': [],
        'missing_values_by_column': defaultdict(int),
        'total_annotations_by_class': defaultdict(int),
        'files_with_imbalanced_classes': [],
        'files_with_highly_imbalanced_classes': [],
        'files_with_problematic_annotations': [],
        'files_missing_annotation_column': [],
        'unique_well_ids': set(),
        'total_segments_by_well': defaultdict(int),
        'files_failed_well_id_extraction': [],
        'files_with_duplicated_segments': [],
        'files_with_frame_gaps': [],
        'files_failed_frame_extraction': [],
        'files_missing_names_column': [],
        'missing_extracted_metadata': defaultdict(int),
    }

    found_files = []
    for root, dirs, files in os.walk(base_dir):
        for file in files:
            if file.startswith('dataset') and file.endswith('.csv'):
                full_path = os.path.join(root, file)
                found_files.append(full_path)

    if not found_files:
        print(f"Nie znaleziono ≈ºadnych plik√≥w 'dataset*.csv' w '{base_dir}' i jego podfolderach.")
        print("--- ANALIZA PROJEKTU ZAKO≈ÉCZONA ---")
        return

    print(f"Znaleziono {len(found_files)} plik√≥w do analizy.")

    for file_path in found_files:
        if analyze_metadata_and_annotations(file_path, project_summary):
            project_summary['total_files_processed'] += 1

    # --- Generowanie sumarycznego raportu ---
    print("\n" + "="*80)
    print("                 RAPORT JAKO≈öCI DANYCH DLA CA≈ÅEGO PROJEKTU                 ")
    print("="*80)

    print(f"\n## üìä Og√≥lne Statystyki Projektu")
    print(f"   - ≈ÅƒÖczna liczba znalezionych plik√≥w CSV (dataset*.csv): **{project_summary['total_files_found']}**")
    print(f"   - ≈ÅƒÖczna liczba przetworzonych plik√≥w: **{project_summary['total_files_processed']}**")
    print(f"   - ≈ÅƒÖczna liczba wierszy (segment√≥w): **{project_summary['total_rows']}**")
    print(f"   - ≈ÅƒÖczna liczba unikalnych studzienek (Well ID): **{len(project_summary['unique_well_ids'])}**")
    if project_summary['unique_well_ids']:
        print(f"     (ID: {', '.join(sorted(list(project_summary['unique_well_ids'])))})")
    
    print(f"\n## ‚ö†Ô∏è Podsumowanie Problem√≥w Jako≈õciowych")
    
    if project_summary['files_processed_with_errors']:
        print(f"   - Pliki, kt√≥re spowodowa≈Çy b≈ÇƒÖd podczas przetwarzania: **{len(project_summary['files_processed_with_errors'])}**")
        for f in project_summary['files_processed_with_errors']:
            print(f"     - {f}")
    
    if project_summary['files_empty']:
        print(f"   - Puste pliki: **{len(project_summary['files_empty'])}**")
        for f in project_summary['files_empty']:
            print(f"     - {f}")

    if project_summary['files_with_column_warnings']:
        print(f"   - Pliki z brakujƒÖcymi oczekiwanymi kolumnami ('names', 'annotation'): **{len(project_summary['files_with_column_warnings'])}**")
        for f in project_summary['files_with_column_warnings']:
            print(f"     - {f}")
            
    if project_summary['files_missing_annotation_column']:
        print(f"   - Pliki bez kolumny 'annotation': **{len(project_summary['files_missing_annotation_column'])}**")
        for f in project_summary['files_missing_annotation_column']:
            print(f"     - {f}")

    if project_summary['files_missing_names_column']:
        print(f"   - Pliki bez kolumny 'names': **{len(project_summary['files_missing_names_column'])}**")
        for f in project_summary['files_missing_names_column']:
            print(f"     - {f}")


    print(f"\n### 1. BrakujƒÖce Warto≈õci")
    if project_summary['files_with_missing_values']:
        print(f"   - Pliki zawierajƒÖce brakujƒÖce warto≈õci: **{len(project_summary['files_with_missing_values'])}**")
        for f in project_summary['files_with_missing_values']:
            print(f"     - {f}")
        print(f"   - Sumaryczna liczba brakujƒÖcych warto≈õci w kolumnach:")
        for col, count in project_summary['missing_values_by_column'].items():
            print(f"     - '{col}': {count}")
    else:
        print("   - Brak brakujƒÖcych warto≈õci we wszystkich przetworzonych plikach. ‚úÖ")

    print(f"\n### 2. Jako≈õƒá Adnotacji Behawioralnych")
    if project_summary['total_annotations_by_class']:
        print(f"   - ≈ÅƒÖczny rozk≈Çad klas adnotacji w ca≈Çym projekcie:")
        total_annotations_df = pd.Series(project_summary['total_annotations_by_class']).sort_values(ascending=False)
        print(total_annotations_df)

        if project_summary['files_with_highly_imbalanced_classes']:
            print(f"   - Pliki z **mocno niezbalansowanymi** klasami: **{len(project_summary['files_with_highly_imbalanced_classes'])}** ‚ö†Ô∏è")
            for f in project_summary['files_with_highly_imbalanced_classes']:
                print(f"     - {f}")
        
        if project_summary['files_with_imbalanced_classes']:
            print(f"   - Pliki z **niezbalansowanymi** klasami (ale nie mocno): **{len(project_summary['files_with_imbalanced_classes'])}** ‚ö†Ô∏è")
            for f in project_summary['files_with_imbalanced_classes']:
                print(f"     - {f}")
        
        if project_summary['files_with_problematic_annotations']:
            print(f"   - Pliki z **problematicznym formatowaniem adnotacji** (np. wiele adnotacji w jednej kom√≥rce): **{len(project_summary['files_with_problematic_annotations'])}** ‚ö†Ô∏è")
            for f in project_summary['files_with_problematic_annotations']:
                print(f"     - {f}")
    else:
        print("   - Brak danych o adnotacjach (mo≈ºe brak kolumny 'annotation' lub puste pliki).")

    print(f"\n### 3. Jako≈õƒá Metadanych z Nazw Plik√≥w")
    if project_summary['total_segments_by_well']:
        print(f"   - ≈ÅƒÖczna liczba segment√≥w na studzienkƒô w ca≈Çym projekcie:")
        total_segments_well_df = pd.Series(project_summary['total_segments_by_well']).sort_values(ascending=False)
        print(total_segments_well_df)
    else:
        print("   - Brak danych o segmentach na studzienkƒô (mo≈ºe brak kolumny 'names' lub brak 'well_id' w nazwach).")

    if project_summary['files_failed_well_id_extraction']:
        print(f"   - Pliki, w kt√≥rych **nie uda≈Ço siƒô wyodrƒôbniƒá Well ID**: **{len(project_summary['files_failed_well_id_extraction'])}** ‚ö†Ô∏è")
        for f in project_summary['files_failed_well_id_extraction']:
            print(f"     - {f}")
            
    if project_summary['files_with_duplicated_segments']:
        print(f"   - Pliki zawierajƒÖce **zduplikowane wpisy segment√≥w**: **{len(project_summary['files_with_duplicated_segments'])}** ‚ö†Ô∏è")
        for f in project_summary['files_with_duplicated_segments']:
            print(f"     - {f}")
            
    if project_summary['files_with_frame_gaps']:
        print(f"   - Pliki, w kt√≥rych znaleziono **luki lub nak≈Çadanie siƒô klatek** miƒôdzy segmentami: **{len(project_summary['files_with_frame_gaps'])}** ‚ö†Ô∏è")
        for f in project_summary['files_with_frame_gaps']:
            print(f"     - {f}")

    if project_summary['files_failed_frame_extraction']:
        print(f"   - Pliki, w kt√≥rych **nie uda≈Ço siƒô wyodrƒôbniƒá zakres√≥w klatek**: **{len(project_summary['files_failed_frame_extraction'])}** ‚ö†Ô∏è")
        for f in project_summary['files_failed_frame_extraction']:
            print(f"     - {f}")
            
    if project_summary['missing_extracted_metadata']:
        print(f"   - Sumaryczna liczba brakujƒÖcych warto≈õci w wyodrƒôbnionych metadanych (date, age_dpf, experiment_type):")
        for col, count in project_summary['missing_extracted_metadata'].items():
            print(f"     - '{col}': {count}")
    else:
        print("   - Brak brakujƒÖcych warto≈õci w wyodrƒôbnionych metadanych (je≈õli kolumny 'names' by≈Çy obecne). ‚úÖ")

    print("\n" + "="*80)
    print("                 ANALIZA PROJEKTU ZAKO≈ÉCZONA                 ")
    print("="*80)

# --- PRZYK≈ÅADOWE U≈ªYCIE ---
if __name__ == "__main__":
    # Ustaw ≈õcie≈ºkƒô do g≈Ç√≥wnego katalogu Twojego projektu
    # Na przyk≈Çad, je≈õli Tw√≥j katalog projektu to 'seizure_detection_ml_dataset'
    # i tam sƒÖ podfoldery z plikami CSV, ustaw:
    project_base_directory = 'seizure_detection_ml_dataset' 
    # Mo≈ºesz te≈º u≈ºyƒá '.' je≈õli skrypt jest uruchamiany z katalogu g≈Ç√≥wnego projektu

    # Tworzenie przyk≈Çadowych plik√≥w dla test√≥w
    print("Tworzenie przyk≈Çadowych plik√≥w do test√≥w...")
    os.makedirs(os.path.join(project_base_directory, 'timepoint1', 'exp1'), exist_ok=True)
    os.makedirs(os.path.join(project_base_directory, 'timepoint2', 'exp2'), exist_ok=True)

    # Przyk≈Çadowy plik 1 (dobra jako≈õƒá)
    pd.DataFrame({
        'names': ['20221205_AB_PTZ_5DPF_Timepoint1_96wp_20221205_161742_024_well_B1_frames_0_to_60.mp4',
                  '20221205_AB_PTZ_5DPF_Timepoint1_96wp_20221205_161742_024_well_B1_frames_60_to_120.mp4'],
        'annotation': ['seizure', 'control']
    }).to_csv(os.path.join(project_base_directory, 'timepoint1', 'exp1', 'dataset_wellB1_annotations.csv'), index=False)

    # Przyk≈Çadowy plik 2 (z brakujƒÖcymi warto≈õciami i niezbalansowanymi klasami)
    pd.DataFrame({
        'names': ['20221206_Control_6DPF_Timepoint1_96wp_20221206_100000_001_well_A1_frames_0_to_60.mp4',
                  '20221206_Control_6DPF_Timepoint1_96wp_20221206_100000_001_well_A1_frames_60_to_120.mp4',
                  '20221206_Control_6DPF_Timepoint1_96wp_20221206_100000_001_well_A1_frames_120_to_180.mp4'],
        'annotation': ['control', 'control', None] # BrakujƒÖca warto≈õƒá
    }).to_csv(os.path.join(project_base_directory, 'timepoint2', 'exp2', 'dataset_wellA1_annotations.csv'), index=False)

    # Przyk≈Çadowy plik 3 (z b≈Çƒôdnym formatowaniem adnotacji i lukƒÖ w klatkach)
    pd.DataFrame({
        'names': ['20221207_AB_PTZ_7DPF_Timepoint1_96wp_20221207_140000_005_well_C3_frames_0_to_60.mp4',
                  '20221207_AB_PTZ_7DPF_Timepoint1_96wp_20221207_140000_005_well_C3_frames_180_to_240.mp4'], # Luka
        'annotation': ['seizure', 'control,seizure'] # B≈Çƒôdne formatowanie
    }).to_csv(os.path.join(project_base_directory, 'timepoint1', 'exp1', 'dataset_wellC3_annotations.csv'), index=False)
    
    # Przyk≈Çadowy plik 4 (pusty)
    open(os.path.join(project_base_directory, 'timepoint1', 'exp1', 'dataset_empty.csv'), 'a').close()

    # Przyk≈Çadowy plik 5 (brak kolumn 'names' i 'annotation')
    pd.DataFrame({
        'other_column': [1,2,3]
    }).to_csv(os.path.join(project_base_directory, 'timepoint2', 'exp2', 'dataset_no_columns.csv'), index=False)


    print("\nRozpoczynanie sumarycznej analizy projektu...")
    summarize_project_data_quality(project_base_directory)

    # Opcjonalnie: usuniƒôcie przyk≈Çadowych plik√≥w po zako≈Ñczeniu
    # import shutil
    # shutil.rmtree(project_base_directory)
    # print(f"\nUsuniƒôto przyk≈Çadowy katalog: {project_base_directory}")